{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key=\"bad6b8a0ac53c6665bbf6201ac36a3ab180041b7\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:01:32.251212Z","iopub.execute_input":"2025-04-07T17:01:32.251527Z","iopub.status.idle":"2025-04-07T17:01:41.106666Z","shell.execute_reply.started":"2025-04-07T17:01:32.251502Z","shell.execute_reply":"2025-04-07T17:01:41.105974Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshravang\u001b[0m (\u001b[33mshravang-iiit-hyderabad\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"# XLM Training without PEFT without permutation","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom datasets import load_dataset, Dataset, load_from_disk, concatenate_datasets\nfrom transformers import (\n    XLMRobertaForSequenceClassification,\n    XLMRobertaTokenizerFast,\n    Trainer,\n    TrainingArguments,\n    TrainerCallback\n)\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import pearsonr, spearmanr\nfrom tqdm.auto import tqdm\n\n##############################################\n# Custom Epoch-Level Progress Callback       #\n##############################################\nclass SingleEpochProgressCallback(TrainerCallback):\n    def on_epoch_begin(self, args, state, control, **kwargs):\n        print(f\"\\nStarting epoch {state.epoch:.0f}/{args.num_train_epochs}\")\n        # Calculate steps per epoch if available.\n        if state.max_steps and args.num_train_epochs:\n            self.steps_per_epoch = int(state.max_steps / args.num_train_epochs)\n        else:\n            self.steps_per_epoch = 0\n        self.progress_bar = tqdm(total=self.steps_per_epoch, desc=f\"Epoch {state.epoch:.0f}\")\n    def on_step_end(self, args, state, control, **kwargs):\n        if hasattr(self, \"progress_bar\"):\n            self.progress_bar.update(1)\n    def on_epoch_end(self, args, state, control, **kwargs):\n        if hasattr(self, \"progress_bar\"):\n            self.progress_bar.close()\n\n##############################################\n# Device and Dataset Paths                   #\n##############################################\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\nlanguages = ['de','en','es','fr','it','nl','pl','pt','ru','zh']\ntrain_path = \"/kaggle/working/combined_train\"\nval_path   = \"/kaggle/working/combined_dev\"\n\n##############################################\n# Dataset Loading & Combining                #\n##############################################\ndef load_and_combine_split(split_name):\n    print(f\"Loading split '{split_name}' for all languages...\")\n    datasets_list = []\n    for lang in languages:\n        print(f\"Loading language {lang}...\")\n        ds = load_dataset(\"PhilipMay/stsb_multi_mt\", lang, split=split_name)\n        datasets_list.append(ds)\n    print(\"Concatenating datasets from all languages...\")\n    combined_dataset = concatenate_datasets(datasets_list)\n    return combined_dataset\n\nif os.path.exists(train_path):\n    train_dataset = load_from_disk(train_path)\n    print(\"Loaded combined train dataset from disk.\")\nelse:\n    print(\"Loading and combining training split...\")\n    train_dataset = load_and_combine_split(\"train\")\n    train_dataset.save_to_disk(train_path)\n    print(\"Saved combined train dataset to disk.\")\n\nif os.path.exists(val_path):\n    val_dataset = load_from_disk(val_path)\n    print(\"Loaded combined validation dataset from disk.\")\nelse:\n    print(\"Loading and combining validation split...\")\n    val_dataset = load_and_combine_split(\"dev\")\n    val_dataset.save_to_disk(val_path)\n    print(\"Saved combined validation dataset to disk.\")\n\n# Optionally set dataset format to torch.\ntrain_dataset.set_format(\"torch\")\nval_dataset.set_format(\"torch\")\nprint(\"Train dataset size:\", len(train_dataset))\nprint(\"Validation dataset size:\", len(val_dataset))\n\n##############################################\n# Model and Tokenizer                        #\n##############################################\nmodel_name = \"xlm-roberta-base\"\ntokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n\ndef model_init():\n    model = XLMRobertaForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=1,\n        problem_type=\"regression\"\n    )\n    model.config.use_cache = False  # Disable cache to avoid warnings.\n    model.to(device)\n    return model\n\n##############################################\n# Preprocessing & Tokenization               #\n##############################################\ndef preprocess_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, max_length=128)\n\nprint(\"Tokenizing train dataset...\")\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\nprint(\"Tokenizing validation dataset...\")\nval_dataset = val_dataset.map(preprocess_function, batched=True)\n\ndef set_labels(example):\n    example[\"labels\"] = float(example[\"similarity_score\"])\n    return example\n\nprint(\"Setting labels for train dataset...\")\ntrain_dataset = train_dataset.map(set_labels)\nprint(\"Setting labels for validation dataset...\")\nval_dataset = val_dataset.map(set_labels)\n\n##############################################\n# Metrics Calculation                        #\n##############################################\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.flatten()\n    pearson_corr = pearsonr(predictions, labels)[0]\n    spearman_corr = spearmanr(predictions, labels)[0]\n    mse = mean_squared_error(labels, predictions)\n    pred_array = np.array(predictions)\n    label_array = np.array(labels)\n    dot = np.dot(pred_array, label_array)\n    norm_pred = np.linalg.norm(pred_array)\n    norm_label = np.linalg.norm(label_array)\n    cosine_sim = dot / (norm_pred * norm_label) if norm_pred and norm_label else 0.0\n    return {\n        \"pearson\": pearson_corr,\n        \"spearman\": spearman_corr,\n        \"mse\": mse,\n        \"cosine\": cosine_sim,\n        \"avg_corr\": (pearson_corr + spearman_corr) / 2\n    }\n\n##############################################\n# Training Arguments                         #\n##############################################\ntraining_args = TrainingArguments(\n    output_dir=\"./xlmroberta_sts_finetuned\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    fp16=True,\n    save_total_limit=2,\n    dataloader_num_workers=4\n)\n\nprint(\"Starting training for XLM-R...\")\ntrainer = Trainer(\n    model_init=model_init,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)\ntrainer.add_callback(SingleEpochProgressCallback())\n\ntrainer.train()\neval_results = trainer.evaluate()\nprint(\"XLM-R Evaluation results:\", eval_results)\n\n# Save the final fine-tuned model without quantization.\nfinal_model = trainer.model\nfinal_model.eval()\nfinal_model.save_pretrained(\"./xlmroberta_sts_finetuned\")\nprint(\"Final fine-tuned XLM-R model saved at './xlmroberta_sts_finetuned'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:05:39.043755Z","iopub.execute_input":"2025-04-07T17:05:39.044088Z","iopub.status.idle":"2025-04-07T17:41:19.889414Z","shell.execute_reply.started":"2025-04-07T17:05:39.044063Z","shell.execute_reply":"2025-04-07T17:41:19.888170Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded combined train dataset from disk.\nLoaded combined validation dataset from disk.\nTrain dataset size: 57490\nValidation dataset size: 15000\nTokenizing train dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/57490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Tokenizing validation dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Setting labels for train dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/57490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Setting labels for validation dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Starting training for XLM-R...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-3-2026d48e7f0c>:160: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nStarting epoch 0/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 0:   0%|          | 0/3594 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10782' max='10782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10782/10782 34:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Pearson</th>\n      <th>Spearman</th>\n      <th>Mse</th>\n      <th>Cosine</th>\n      <th>Avg Corr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.765600</td>\n      <td>0.580416</td>\n      <td>0.869315</td>\n      <td>0.864950</td>\n      <td>0.580416</td>\n      <td>0.964246</td>\n      <td>0.867132</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.321600</td>\n      <td>0.583129</td>\n      <td>0.866318</td>\n      <td>0.864279</td>\n      <td>0.583129</td>\n      <td>0.962234</td>\n      <td>0.865298</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.192500</td>\n      <td>0.574017</td>\n      <td>0.869220</td>\n      <td>0.866355</td>\n      <td>0.574017</td>\n      <td>0.964183</td>\n      <td>0.867787</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nStarting epoch 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/3594 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nStarting epoch 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/3594 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [938/938 00:30]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"XLM-R Evaluation results: {'eval_loss': 0.5740170478820801, 'eval_pearson': 0.869220029663228, 'eval_spearman': 0.8663548553431225, 'eval_mse': 0.5740170478820801, 'eval_cosine': 0.9641826152801514, 'eval_avg_corr': 0.8677874425031753, 'eval_runtime': 30.4101, 'eval_samples_per_second': 493.257, 'eval_steps_per_second': 30.845, 'epoch': 3.0}\nFinal fine-tuned XLM-R model saved at './xlmroberta_sts_finetuned'.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Analysis without PEFT without permutation on test set with csv generation","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom datasets import load_from_disk, load_dataset, concatenate_datasets, Dataset\nfrom transformers import (\n    XLMRobertaForSequenceClassification,\n    XLMRobertaTokenizerFast,\n    Trainer,\n    TrainingArguments,\n    XLMRobertaConfig\n)\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.auto import tqdm\nimport logging\n\n# Suppress transformer warnings.\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n##############################################\n# Define Languages and Concatenation Function#\n##############################################\nlanguages = ['de','en','es','fr','it','nl','pl','pt','ru','zh']\n\ndef load_and_concatenate_split(split_name):\n    print(f\"Loading split '{split_name}' for all languages...\")\n    datasets_list = []\n    for lang in languages:\n        print(f\"Loading language {lang}...\")\n        ds = load_dataset(\"PhilipMay/stsb_multi_mt\", lang, split=split_name)\n        datasets_list.append(ds)\n    print(\"Concatenating datasets from all languages...\")\n    combined_dataset = concatenate_datasets(datasets_list)\n    return combined_dataset\n\n##############################################\n# Matching Configuration                     #\n##############################################\nconfig = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\")\nconfig.num_labels = 1\nconfig.problem_type = \"regression\"\nconfig.use_cache = False\n# Set these to match your training checkpoint dimensions.\nconfig.vocab_size = 250002\nconfig.max_position_embeddings = 514\nconfig.type_vocab_size = 1\n\n##############################################\n# Load the Fine-Tuned Model (Non-Quantized)    #\n##############################################\n# IMPORTANT: To use CUDA, load a non-quantized version of your model.\nmodel_path = \"/kaggle/working/xlmroberta_sts_finetuned\"  # use the non-quantized checkpoint here\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_path, config=config)\nmodel.to(device)\nmodel.eval()\n\n##############################################\n# Load the Tokenizer                         #\n##############################################\ntokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n\n##############################################\n# Load and Prepare the Test Dataset          #\n##############################################\ntest_path = \"/kaggle/working/combined_test\"\nif os.path.exists(test_path):\n    test_dataset = load_from_disk(test_path)\n    print(\"Loaded combined test dataset from disk.\")\nelse:\n    print(\"Combined test dataset not found on disk; generating concatenated test dataset on the fly...\")\n    test_dataset = load_and_concatenate_split(\"test\")\n    test_dataset.save_to_disk(\"/kaggle/working/combined_test\")\n\n# Tokenize test data if needed.\nif \"input_ids\" not in test_dataset.column_names:\n    def preprocess_function(example):\n        return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, max_length=128)\n    test_dataset = test_dataset.map(preprocess_function, batched=True, desc=\"Tokenizing\")\n\n# Ensure the labels are correctly set.\nif \"labels\" not in test_dataset.column_names:\n    def set_labels(example):\n        example[\"labels\"] = [float(example[\"similarity_score\"])]\n        return example\n    test_dataset = test_dataset.map(set_labels, desc=\"Setting labels\")\n\ntest_dataset.set_format(\"torch\")\n\n##############################################\n# Setup Trainer (Using GPU)                  #\n##############################################\ntraining_args = TrainingArguments(\n    output_dir=\"./eval_results\",\n    per_device_eval_batch_size=16,\n    logging_strategy=\"no\",\n    report_to=None\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer\n)\n\n##############################################\n# Manual Evaluation Loop with Progress Bar   #\n##############################################\neval_dataloader = trainer.get_eval_dataloader()\nall_preds = []\n\nfor batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n    # Move batch to GPU.\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n    all_preds.append(outputs.logits.cpu().numpy())\n\npreds = np.concatenate(all_preds).flatten()\n\n# Also get gold scores and original sentences.\ngold_scores = np.array(test_dataset[\"similarity_score\"])\nsentences1 = test_dataset[\"sentence1\"]\nsentences2 = test_dataset[\"sentence2\"]\n\n##############################################\n# Generate CSV with Predictions              #\n##############################################\ndf = pd.DataFrame({\n    \"sentence1\": sentences1,\n    \"sentence2\": sentences2,\n    \"gold_similarity_score\": gold_scores,\n    \"predicted_similarity_score\": preds\n})\n\noutput_csv_path = \"./evaluation_results.csv\"\ndf.to_csv(output_csv_path, index=False)\nprint(f\"CSV file with evaluation results saved at: {output_csv_path}\")\n\n##############################################\n# Compute Metrics (Optional)                 #\n##############################################\ndef compute_metrics_from_preds(preds, gold):\n    pearson_corr = pearsonr(preds, gold)[0]\n    spearman_corr = spearmanr(preds, gold)[0]\n    mse = mean_squared_error(gold, preds)\n    dot = np.dot(preds, gold)\n    norm_pred = np.linalg.norm(preds)\n    norm_gold = np.linalg.norm(gold)\n    cosine_sim = dot / (norm_pred * norm_gold) if norm_pred and norm_gold else 0.0\n    avg_corr = (pearson_corr + spearman_corr) / 2.0\n    return {\n        \"pearson\": pearson_corr,\n        \"spearman\": spearman_corr,\n        \"mse\": mse,\n        \"cosine\": cosine_sim,\n        \"avg_corr\": avg_corr\n    }\n\nmetrics = compute_metrics_from_preds(preds, gold_scores)\nprint(\"Evaluation Metrics:\")\nprint(metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:05:37.888265Z","iopub.execute_input":"2025-04-07T18:05:37.888606Z","iopub.status.idle":"2025-04-07T18:06:26.875021Z","shell.execute_reply.started":"2025-04-07T18:05:37.888571Z","shell.execute_reply":"2025-04-07T18:06:26.874043Z"}},"outputs":[{"name":"stdout","text":"Combined test dataset not found on disk; generating concatenated test dataset on the fly...\nLoading split 'test' for all languages...\nLoading language de...\nLoading language en...\nLoading language es...\nLoading language fr...\nLoading language it...\nLoading language nl...\nLoading language pl...\nLoading language pt...\nLoading language ru...\nLoading language zh...\nConcatenating datasets from all languages...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/13790 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d08a12b3d840f7b8117e942c1d0553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/13790 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0ae54c0c2a14820a53ec63daab4c8ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Setting labels:   0%|          | 0/13790 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c98911401264715bdfbfff285ce9be6"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-6-fc0f73afcdf0>:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/862 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08850ace37ad4d148bafa47e09a57646"}},"metadata":{}},{"name":"stdout","text":"CSV file with evaluation results saved at: ./evaluation_results.csv\nEvaluation Metrics:\n{'pearson': 0.8272604417102964, 'spearman': 0.8168667922807934, 'mse': 0.7521109, 'cosine': 0.9588413, 'avg_corr': 0.8220636169955449}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Analysis without PEFT without permutation on dev set without csv generation","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom datasets import load_from_disk, load_dataset, concatenate_datasets, Dataset\nfrom transformers import (\n    XLMRobertaForSequenceClassification,\n    XLMRobertaTokenizerFast,\n    Trainer,\n    TrainingArguments,\n    XLMRobertaConfig\n)\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.auto import tqdm\nimport logging\n\n# Suppress transformer warnings.\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n##############################################\n# Define Languages and Concatenation Function#\n##############################################\nlanguages = ['de','en','es','fr','it','nl','pl','pt','ru','zh']\n\ndef load_and_concatenate_split(split_name):\n    print(f\"Loading split '{split_name}' for all languages...\")\n    datasets_list = []\n    for lang in languages:\n        print(f\"Loading language {lang}...\")\n        ds = load_dataset(\"PhilipMay/stsb_multi_mt\", lang, split=split_name)\n        datasets_list.append(ds)\n    print(\"Concatenating datasets from all languages...\")\n    combined_dataset = concatenate_datasets(datasets_list)\n    return combined_dataset\n\n##############################################\n# Matching Configuration                     #\n##############################################\nconfig = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\")\nconfig.num_labels = 1\nconfig.problem_type = \"regression\"\nconfig.use_cache = False\n# Set these to match your training checkpoint dimensions.\nconfig.vocab_size = 250002\nconfig.max_position_embeddings = 514\nconfig.type_vocab_size = 1\n\n##############################################\n# Load the Fine-Tuned Model (Non-Quantized)    #\n##############################################\n# IMPORTANT: To use CUDA, load a non-quantized version of your model.\nmodel_path = \"/kaggle/working/xlmroberta_sts_finetuned\"  # use the non-quantized checkpoint here\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_path, config=config)\nmodel.to(device)\nmodel.eval()\n\n##############################################\n# Load the Tokenizer                         #\n##############################################\ntokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n\n##############################################\n# Load and Prepare the Dev Dataset           #\n##############################################\ndev_path = \"/kaggle/working/combined_dev\"\nif os.path.exists(dev_path):\n    dev_dataset = load_from_disk(dev_path)\n    print(\"Loaded combined dev dataset from disk.\")\nelse:\n    print(\"Combined dev dataset not found on disk; generating concatenated dev dataset on the fly...\")\n    dev_dataset = load_and_concatenate_split(\"dev\")\n    dev_dataset.save_to_disk(\"/kaggle/working/combined_dev\")\n\n# Tokenize dev data if needed.\nif \"input_ids\" not in dev_dataset.column_names:\n    def preprocess_function(example):\n        return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, max_length=128)\n    dev_dataset = dev_dataset.map(preprocess_function, batched=True, desc=\"Tokenizing\")\n\n# Ensure the labels are correctly set.\nif \"labels\" not in dev_dataset.column_names:\n    def set_labels(example):\n        example[\"labels\"] = [float(example[\"similarity_score\"])]\n        return example\n    dev_dataset = dev_dataset.map(set_labels, desc=\"Setting labels\")\n\ndev_dataset.set_format(\"torch\")\n\n##############################################\n# Setup Trainer (Using GPU)                  #\n##############################################\ntraining_args = TrainingArguments(\n    output_dir=\"./eval_results\",\n    per_device_eval_batch_size=16,\n    logging_strategy=\"no\",\n    report_to=None\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    eval_dataset=dev_dataset,\n    tokenizer=tokenizer\n)\n\n##############################################\n# Manual Evaluation Loop with Progress Bar   #\n##############################################\neval_dataloader = trainer.get_eval_dataloader()\nall_preds = []\n\nfor batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n    # Move batch to GPU.\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n    all_preds.append(outputs.logits.cpu().numpy())\n\npreds = np.concatenate(all_preds).flatten()\n\n# Also get gold scores.\ngold_scores = np.array(dev_dataset[\"similarity_score\"])\n\n##############################################\n# Compute Metrics                            #\n##############################################\ndef compute_metrics_from_preds(preds, gold):\n    pearson_corr = pearsonr(preds, gold)[0]\n    spearman_corr = spearmanr(preds, gold)[0]\n    mse = mean_squared_error(gold, preds)\n    dot = np.dot(preds, gold)\n    norm_pred = np.linalg.norm(preds)\n    norm_gold = np.linalg.norm(gold)\n    cosine_sim = dot / (norm_pred * norm_gold) if norm_pred and norm_gold else 0.0\n    avg_corr = (pearson_corr + spearman_corr) / 2.0\n    return {\n        \"pearson\": pearson_corr,\n        \"spearman\": spearman_corr,\n        \"mse\": mse,\n        \"cosine\": cosine_sim,\n        \"avg_corr\": avg_corr\n    }\n\nmetrics = compute_metrics_from_preds(preds, gold_scores)\nprint(\"Evaluation Metrics on Dev Set:\")\nprint(metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:25:45.654236Z","iopub.execute_input":"2025-04-07T18:25:45.654655Z","iopub.status.idle":"2025-04-07T18:26:20.978328Z","shell.execute_reply.started":"2025-04-07T18:25:45.654608Z","shell.execute_reply":"2025-04-07T18:26:20.977420Z"}},"outputs":[{"name":"stdout","text":"Loaded combined dev dataset from disk.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Setting labels:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"<ipython-input-7-b304b028a74e>:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/938 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Evaluation Metrics on Dev Set:\n{'pearson': 0.8691959427411435, 'spearman': 0.8663298933496116, 'mse': 0.5742456, 'cosine': 0.9641784, 'avg_corr': 0.8677629180453776}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!zip -r XLM_trained.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:44:08.944090Z","iopub.execute_input":"2025-04-07T17:44:08.944419Z","iopub.status.idle":"2025-04-07T17:52:19.272464Z","shell.execute_reply.started":"2025-04-07T17:44:08.944391Z","shell.execute_reply":"2025-04-07T17:52:19.271725Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/combined_dev/ (stored 0%)\n  adding: kaggle/working/combined_dev/state.json (deflated 41%)\n  adding: kaggle/working/combined_dev/data-00000-of-00001.arrow (deflated 60%)\n  adding: kaggle/working/combined_dev/cache-c349c6dabdf0f66e.arrow (deflated 65%)\n  adding: kaggle/working/combined_dev/dataset_info.json (deflated 58%)\n  adding: kaggle/working/combined_dev/cache-b05a34031e2bc56c.arrow (deflated 65%)\n  adding: kaggle/working/wandb/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/run-lcjjyjoi.wandb (deflated 79%)\n  adding: kaggle/working/wandb/latest-run/logs/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/logs/debug-core.log (deflated 58%)\n  adding: kaggle/working/wandb/latest-run/logs/debug-internal.log (deflated 71%)\n  adding: kaggle/working/wandb/latest-run/logs/debug.log (deflated 80%)\n  adding: kaggle/working/wandb/latest-run/files/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/files/wandb-metadata.json (deflated 47%)\n  adding: kaggle/working/wandb/latest-run/files/output.log (deflated 64%)\n  adding: kaggle/working/wandb/latest-run/files/requirements.txt (deflated 56%)\n  adding: kaggle/working/wandb/latest-run/tmp/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/tmp/code/ (stored 0%)\n  adding: kaggle/working/wandb/debug-internal.log (deflated 71%)\n  adding: kaggle/working/wandb/debug.log (deflated 80%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/ (stored 0%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/run-lcjjyjoi.wandb (deflated 79%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/logs/ (stored 0%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/logs/debug-core.log (deflated 58%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/logs/debug-internal.log (deflated 71%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/logs/debug.log (deflated 80%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/files/ (stored 0%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/files/wandb-metadata.json (deflated 47%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/files/output.log (deflated 64%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/files/requirements.txt (deflated 56%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/tmp/ (stored 0%)\n  adding: kaggle/working/wandb/run-20250407_170407-lcjjyjoi/tmp/code/ (stored 0%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/ (stored 0%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/ (stored 0%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/training_args.bin (deflated 51%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/scheduler.pt (deflated 55%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/trainer_state.json (deflated 63%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/config.json (deflated 50%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/tokenizer.json (deflated 76%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/optimizer.pt (deflated 59%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/tokenizer_config.json (deflated 76%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/sentencepiece.bpe.model (deflated 49%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/rng_state.pth (deflated 25%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-7188/model.safetensors (deflated 23%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/config.json (deflated 50%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/ (stored 0%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/training_args.bin (deflated 51%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/scheduler.pt (deflated 56%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/trainer_state.json (deflated 67%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/config.json (deflated 50%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/tokenizer.json (deflated 76%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/optimizer.pt (deflated 59%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/tokenizer_config.json (deflated 76%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/sentencepiece.bpe.model (deflated 49%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/rng_state.pth (deflated 25%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/checkpoint-10782/model.safetensors (deflated 23%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/runs/ (stored 0%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/runs/Apr07_17-06-25_140e7e3aa0b4/ (stored 0%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/runs/Apr07_17-06-25_140e7e3aa0b4/events.out.tfevents.1744047677.140e7e3aa0b4.31.2 (deflated 34%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/runs/Apr07_17-06-25_140e7e3aa0b4/events.out.tfevents.1744045587.140e7e3aa0b4.31.1 (deflated 60%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/runs/Apr07_17-04-01_140e7e3aa0b4/ (stored 0%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/runs/Apr07_17-04-01_140e7e3aa0b4/events.out.tfevents.1744045447.140e7e3aa0b4.31.0 (deflated 61%)\n  adding: kaggle/working/xlmroberta_sts_finetuned/model.safetensors (deflated 23%)\n  adding: kaggle/working/combined_train/ (stored 0%)\n  adding: kaggle/working/combined_train/state.json (deflated 41%)\n  adding: kaggle/working/combined_train/cache-c99758f7ed5316d5.arrow (deflated 68%)\n  adding: kaggle/working/combined_train/data-00000-of-00001.arrow (deflated 63%)\n  adding: kaggle/working/combined_train/dataset_info.json (deflated 58%)\n  adding: kaggle/working/combined_train/cache-bfe9637d616a29b6.arrow (deflated 68%)\n","output_type":"stream"}],"execution_count":4}]}